{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval Pattern Matchers on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score as f1, accuracy_score as acc, precision_score as prec, recall_score as rec, matthews_corrcoef as mcc\n",
    "import numpy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "# from spellchecker import SpellChecker\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "sigdig = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'askparents'\n",
    "\n",
    "train = pd.read_csv('../../annotated_data/' + data + '_train.tsv', sep='\\t', header=0)\n",
    "train['Sentence'] = train['Sentence'].apply(lambda x: x.lower())\n",
    "train_sentences = train['Sentence'].tolist()\n",
    "train_labels_DS = train['DS_Label'].values\n",
    "train_labels_Maj = train['Majority_label'].values\n",
    "\n",
    "\n",
    "dev = pd.read_csv('../../annotated_data/' + data + '_dev.tsv', sep='\\t', header=0)\n",
    "dev['Sentence'] = dev['Sentence'].apply(lambda x: x.lower())\n",
    "dev_sentences = dev['Sentence'].tolist()\n",
    "dev_labels_DS = dev['DS_Label'].values\n",
    "dev_labels_Maj = dev['Majority_label'].values\n",
    "\n",
    "test = pd.read_csv('../../annotated_data/' + data + '_test.tsv', sep='\\t', header=0)\n",
    "test['Sentence'] = test['Sentence'].apply(lambda x: x.lower())\n",
    "test_sentences_ap = test['Sentence'].tolist()\n",
    "test_labels_DS_ap = test['DS_Label'].values\n",
    "test_labels_Maj_ap = test['Majority_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is advice, 0 is not.\n",
      "Distribution of Train set: Counter({0: 6205, 1: 2496}) 0.29\n",
      "Distribution of Dev set: Counter({0: 534, 1: 268}) 0.33\n",
      "Distribution of Test set: Counter({0: 806, 1: 285}) 0.26\n"
     ]
    }
   ],
   "source": [
    "print(\"1 is advice, 0 is not.\")\n",
    "print(\"Distribution of Train set:\", Counter(train_labels_DS), np.round(Counter(train_labels_DS)[1]/len(train_labels_DS),2))\n",
    "print(\"Distribution of Dev set:\", Counter(dev_labels_DS),  np.round(Counter(dev_labels_DS)[1]/len(dev_labels_DS), 2))\n",
    "print(\"Distribution of Test set:\", Counter(test_labels_DS_ap),  np.round(Counter(test_labels_DS_ap)[1]/len(test_labels_DS_ap), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Post.ID'] = train['ID'].apply(lambda x: x.split('-')[0])\n",
    "print(len(train['Post.ID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemEval Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sent_list):\n",
    "\n",
    "    keywords = [\"suggest\",\"recommend\",\"hopefully\",\"go for\",\"request\",\"it would be nice\",\"adding\",\n",
    "                \"should come with\",\"should be able\",\"could come with\", \"i need\" , \"we need\",\"needs\", \n",
    "                \"would like to\",\"would love to\",\"allow\",\"add\"]\n",
    "\n",
    "    # Goldberg et al.\n",
    "    pattern_strings = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*i\\swant.*', \n",
    "                       r'.*hopefully.*', r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\",\n",
    "                       r\".*would\\sthat.*\", r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", \n",
    "                       r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "    compiled_patterns = []\n",
    "    for patt in pattern_strings:\n",
    "        compiled_patterns.append(re.compile(patt))\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    for sent in sent_list:\n",
    "        tokenized_sent = word_tokenize(sent)\n",
    "        tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "        tags = [i[1] for i in tagged_sent]\n",
    "        label = 0\n",
    "        patt_matched = False\n",
    "        for compiled_patt in compiled_patterns:\n",
    "            joined_sent = \" \".join(tokenized_sent)\n",
    "            matches = compiled_patt.findall(joined_sent)\n",
    "            if len(matches) > 0:\n",
    "                patt_matched = True\n",
    "        keyword_match = any(elem in keywords for elem in tokenized_sent)\n",
    "\n",
    "        pos_match = any(elem in ['MD', 'VB'] for elem in tags)\n",
    "\n",
    "        if patt_matched:\n",
    "            label = 1\n",
    "        elif keyword_match == True:\n",
    "            label = 1\n",
    "        elif pos_match == True:\n",
    "            label = 1\n",
    "\n",
    "        label_list.append(label)\n",
    "\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.489\n",
      "MCC:  0.123\n",
      "Acc:  0.527\n",
      "Precision:  0.383\n",
      "Recall:  0.675\n"
     ]
    }
   ],
   "source": [
    "dev_pred_labels_baseline = classify(dev_sentences)\n",
    "print(\"F1:\", np.round(f1(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"Acc: \", np.round(acc(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"Precision: \", np.round(prec(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"Recall: \", np.round(rec(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 1 F1 on dev: 0.501\n",
      "All 1 precision on dev: 0.334\n",
      "All 1 recall on dev: 1.0\n",
      "All 1 acc on dev: 0.666\n",
      "All 1 mcc on dev: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat/opt/miniconda3/envs/advice/lib/python3.6/site-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "print(\"All 1 F1 on dev:\", np.round(f1(dev_labels_DS, [1 for i in range(len(dev_labels_DS))]), sigdig))\n",
    "print(\"All 1 precision on dev:\", np.round(prec(dev_labels_DS, [1 for i in range(len(dev_labels_DS))]), sigdig))\n",
    "print(\"All 1 recall on dev:\", np.round(rec(dev_labels_DS, [1 for i in range(len(dev_labels_DS))]), sigdig))\n",
    "print(\"All 1 acc on dev:\", np.round(acc(dev_labels_DS, [0 for i in range(len(dev_labels_DS))]), sigdig))\n",
    "print(\"All 1 mcc on dev:\", np.round(mcc(dev_labels_DS, [1 for i in range(len(dev_labels_DS))]), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTUA-IS stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask A Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classify(sent_list, sk, P_ab=True, P_c=True, imperative=True, spelling=False):\n",
    "    \n",
    "    # words from above with other example words they included - P_a\n",
    "    pattern_pa = [\"suggest\",\"recommend\",\"hopefully\",\"go for\",\"request\",\"it would be nice\",\"adding\",\n",
    "                   \"should come with\",\"should be able\",\"could come with\", \"i need\" , \"we need\",\"needs\", \n",
    "                   \"would like to\",\"would love to\",\"allow\",\"add\", \"helpful\", \"allow\", \"disallow\", \"idea\",\n",
    "                   \"consider\"]\n",
    "\n",
    "    # Goldberg et al.\n",
    "    pattern_pc = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*i\\swant.*', \n",
    "                  r'.*hopefully.*', r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\",\n",
    "                  r\".*would\\sthat.*\", r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", \n",
    "                  r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "    \n",
    "    # pattern list P_c rules for subtask A\n",
    "    pattern_pc += [r'.*should\\s(not|be|take|include|start).*', r'.*be\\sbetter.*', r'.*that\\sway.*',\n",
    "                   r'.*so\\sthat.*', r'.*why\\snot.*', r'.*suggestion\\sis.*', r'.*good\\ssolution.*',\n",
    "                   r'.*the\\sidea.*', r'.*to\\sallow.*', r'.*would\\smake.*', r'.*(will|would)\\sbe.*',\n",
    "                   r'.*(to|would|could)\\senable\\s(i|would|id)\\s(like|prefer).*', r'.*am\\sasking\\sfor.*',\n",
    "                   r'.*look\\sinto.*', r'.*make\\sit.*', r'.*at\\sleast.*', r'.*we\\sneed.*']\n",
    "    compiled_pc = [re.compile(patt) for patt in pattern_pc]\n",
    "    \n",
    "    # pattern list P_b rules for subtask B (and possibly the same for subtask A)\n",
    "    # pattern list P_b rules for subtask A\n",
    "    pattern_pb = [r'.*do\\snot.*', r'.*if\\sonly.*', r'.*(so|before|can|for|if)\\syou.*', \n",
    "                   r'.*you\\s(will|need|can|may).*', r'.*(make|be)\\ssure.*', r'.*watch\\sout.*', \n",
    "                   r'.*(go|going|asking|wishing)\\sfor.*', r'.*would\\sadvise.*', \n",
    "                   r'.*(will|would|could)\\sbe.*', r'.*be\\s(prepared|careful|warned|forewarned).*',\n",
    "                   r'.*(i/would/i\\'d)\\s(like|prefer).*', r'.*highly\\srecommended.*', \n",
    "                   r'.*(look|looking)\\s(into|for|up|around).*', r'.*why\\snot.*', r'.*is\\sthere.*',\n",
    "                   r'.*we\\sneed.*']\n",
    "    compiled_pb = [re.compile(patt) for patt in pattern_pb]\n",
    "        \n",
    "    pos_pattern_strings = [r'^UH\\sVBP.*', r'^MD\\sRB\\sPRP.*', r'^(VB|VBP).*', r'^MD.*', \n",
    "                           r'^(DT|RB|PRP|NN)\\sVB.*']\n",
    "    compiled_pos_patterns = [re.compile(patt) for patt in pos_pattern_strings]\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    for sent in sent_list:\n",
    "        score = 0\n",
    "        \n",
    "        if len(sent.split()) < 5:\n",
    "            score -=0.2\n",
    "        \n",
    "        clause_split = [a for a in re.split(\"[.,!?;]|(Please|please)\", sent) if a not in \n",
    "                        [None, '', ' ', 'Please', 'please']]\n",
    "        for clause in clause_split:\n",
    "            clause_pos = TextBlob(clause).tags\n",
    "            \n",
    "            words = [i[0] for i in clause_pos]\n",
    "            tags = [i[1] for i in clause_pos]\n",
    "            \n",
    "            # Correct misspells\n",
    "            if spelling:\n",
    "                words = [spell.correction(w) if w not in spell else w for w in words]\n",
    "            \n",
    "            if P_ab:            \n",
    "                # Pattern P_a\n",
    "                if any(elem in pattern_pa for elem in words):\n",
    "                    score += 0.3\n",
    "\n",
    "                # Pattern P_b\n",
    "                for compiled_patt in compiled_pb:\n",
    "                    joined_sent = \" \".join(words)\n",
    "                    matches = compiled_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += 0.1\n",
    "            if P_c:\n",
    "                # Pattern P_c\n",
    "                for compiled_patt in compiled_pc:\n",
    "                    joined_sent = \" \".join(words)\n",
    "                    matches = compiled_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += 0.25\n",
    "\n",
    "            if imperative:\n",
    "                # Imperative POS pattern check\n",
    "                for compiled_pos_patt in compiled_pos_patterns:\n",
    "                    joined_sent = \" \".join(tags)\n",
    "                    matches = compiled_pos_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += sk\n",
    "\n",
    "        if score > 0:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.25\n",
      "Precision: 0.3787878787878788\n",
      "Recall: 0.1865671641791045\n"
     ]
    }
   ],
   "source": [
    "dev_pred_labels = gr_classify(dev_sentences, sk=0)\n",
    "print(\"F1:\", f1(dev_labels_DS, dev_pred_labels))\n",
    "print(\"Precision:\", prec(dev_labels_DS, dev_pred_labels))\n",
    "print(\"Recall:\", rec(dev_labels_DS, dev_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classify_b(sent_list, pos_s, P_a=True, P_b=True, imperative=True, spelling=False):\n",
    "    \n",
    "    # words from above with other example words they included - P_a\n",
    "    pattern_pa = ['avoid', 'beware', \"don't\", 'expect', 'remember', 'tip', 'advise', 'advice', 'recommended',\n",
    "                  'recommendation', 'suggest', 'suggestion', 'ask', 'bring', 'pick', 'consider', 'spend', \n",
    "                  'expect', 'can', 'please', 'can', 'hopefully', 'enjoying', 'want', 'wanting', 'prefer']\n",
    "    \n",
    "\n",
    "#     # Goldberg et al.\n",
    "    pattern_pc = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*i\\swant.*', \n",
    "                  r'.*hopefully.*', r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\",\n",
    "                  r\".*would\\sthat.*\", r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", \n",
    "                  r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "    \n",
    "    # pattern list P_c rules for subtask A\n",
    "    pattern_pc += [r'.*should\\s(not|be|take|include|start).*', r'.*be\\sbetter.*', r'.*that\\sway.*',\n",
    "                   r'.*so\\sthat.*', r'.*why\\snot.*', r'.*suggestion\\sis.*', r'.*good\\ssolution.*',\n",
    "                   r'.*the\\sidea.*', r'.*to\\sallow.*', r'.*would\\smake.*', r'.*(will|would)\\sbe.*',\n",
    "                   r'.*(to|would|could)\\senable\\s(i|would|id)\\s(like|prefer).*', r'.*am\\sasking\\sfor.*',\n",
    "                   r'.*look\\sinto.*', r'.*make\\sit.*', r'.*at\\sleast.*', r'.*we\\sneed.*']\n",
    "    compiled_pc = [re.compile(patt) for patt in pattern_pc]\n",
    "    \n",
    "    # pattern list P_b rules for subtask B (and possibly the same for subtask A)\n",
    "    # pattern list P_b rules for subtask A\n",
    "    pattern_pb = [r'.*do\\snot.*', r'.*if\\sonly.*', r'.*(so|before|can|for|if)\\syou.*', \n",
    "                   r'.*you\\s(will|need|can|may).*', r'.*(make|be)\\ssure.*', r'.*watch\\sout.*', \n",
    "                   r'.*(go|going|asking|wishing)\\sfor.*', r'.*would\\sadvise.*', \n",
    "                   r'.*(will|would|could)\\sbe.*', r'.*be\\s(prepared|careful|warned|forewarned).*',\n",
    "                   r'.*(i/would/i\\'d)\\s(like|prefer).*', r'.*highly\\srecommended.*', \n",
    "                   r'.*(look|looking)\\s(into|for|up|around).*', r'.*why\\snot.*', r'.*is\\sthere.*',\n",
    "                   r'.*we\\sneed.*']\n",
    "    compiled_pb = [re.compile(patt) for patt in pattern_pb]\n",
    "        \n",
    "    pos_pattern_strings = [r'^UH\\sVBP.*', r'^MD\\sRB\\sPRP.*', r'^(VB|VBP).*', r'^MD.*', \n",
    "                           r'^(DT|RB|PRP|NN)\\sVB.*']\n",
    "    compiled_pos_patterns = [re.compile(patt) for patt in pos_pattern_strings]\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    for sent in sent_list:\n",
    "        score = 0\n",
    "        \n",
    "        if len(sent.split()) < 5:\n",
    "            score -=0.2\n",
    "        \n",
    "        clause_split = [a for a in re.split(\"[.,!?;]|(please)\", sent) if a not in \n",
    "                        [None, '', ' ', 'please']]\n",
    "        for clause in clause_split:\n",
    "            clause_pos = TextBlob(clause).tags\n",
    "            \n",
    "            words = [i[0] for i in clause_pos]\n",
    "            tags = [i[1] for i in clause_pos]\n",
    "            \n",
    "            # Correct misspells\n",
    "            if spelling:\n",
    "                words = [spell.correction(w) if w not in spell else w for w in words]\n",
    "            \n",
    "            if P_a:            \n",
    "                # Pattern P_a\n",
    "                if any(elem in pattern_pa for elem in words):\n",
    "                    score += 0.25\n",
    "\n",
    "\n",
    "            if P_b:\n",
    "                # Pattern P_b\n",
    "                for compiled_patt in compiled_pb:\n",
    "                    joined_sent = \" \".join(words)\n",
    "                    matches = compiled_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += 0.1\n",
    "\n",
    "            if imperative:\n",
    "                # Imperative POS pattern check\n",
    "                for compiled_pos_patt in compiled_pos_patterns:\n",
    "                    joined_sent = \" \".join(tags)\n",
    "                    matches = compiled_pos_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += pos_s\n",
    "\n",
    "        if score > 0.1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.43722304283604135\n",
      "Precision: 0.36185819070904646\n",
      "Recall: 0.5522388059701493\n"
     ]
    }
   ],
   "source": [
    "dev_pred_labels_b = gr_classify_b(dev_sentences, pos_s=0.15)\n",
    "print(\"F1:\", f1(dev_labels_DS, dev_pred_labels_b))\n",
    "print(\"Precision:\", prec(dev_labels_DS, dev_pred_labels_b))\n",
    "print(\"Recall:\", rec(dev_labels_DS, dev_pred_labels_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_classify_all(sent_list, pos_s, P_a=True, P_b=True, P_c=True, imperative=True, spelling=False):\n",
    "    \n",
    "    # words from above with other example words they included - P_a\n",
    "    pattern_pa = [\"suggest\",\"recommend\",\"hopefully\",\"go for\",\"request\",\"it would be nice\",\"adding\",\n",
    "                   \"should come with\",\"should be able\",\"could come with\", \"i need\" , \"we need\",\"needs\", \n",
    "                   \"would like to\",\"would love to\",\"allow\",\"add\", \"helpful\", \"allow\", \"disallow\", \"idea\",\n",
    "                   \"consider\"]\n",
    "    pattern_pa += ['avoid', 'beware', \"don't\", 'expect', 'remember', 'tip', 'advise', 'advice', 'recommended',\n",
    "                  'recommendation', 'suggest', 'suggestion', 'ask', 'bring', 'pick', 'consider', 'spend', \n",
    "                  'expect', 'can', 'please', 'can', 'hopefully', 'enjoying', 'want', 'wanting', 'prefer']\n",
    "    \n",
    "\n",
    "#     # Goldberg et al.\n",
    "    pattern_pc = [r'.*would\\slike.*if.*', r'.*i\\swish.*', r'.*i\\shope.*', r'.*i\\swant.*', \n",
    "                  r'.*hopefully.*', r\".*if\\sonly.*\", r\".*would\\sbe\\sbetter\\sif.*\", r\".*should.*\",\n",
    "                  r\".*would\\sthat.*\", r\".*can't\\sbelieve.*didn't.*\", r\".*don't\\sbelieve.*didn't.*\", \n",
    "                  r\".*do\\swant.*\", r\".*i\\scan\\shas.*\"]\n",
    "    \n",
    "    # pattern list P_c rules for subtask A\n",
    "    pattern_pc += [r'.*should\\s(not|be|take|include|start).*', r'.*be\\sbetter.*', r'.*that\\sway.*',\n",
    "                   r'.*so\\sthat.*', r'.*why\\snot.*', r'.*suggestion\\sis.*', r'.*good\\ssolution.*',\n",
    "                   r'.*the\\sidea.*', r'.*to\\sallow.*', r'.*would\\smake.*', r'.*(will|would)\\sbe.*',\n",
    "                   r'.*(to|would|could)\\senable\\s(i|would|id)\\s(like|prefer).*', r'.*am\\sasking\\sfor.*',\n",
    "                   r'.*look\\sinto.*', r'.*make\\sit.*', r'.*at\\sleast.*', r'.*we\\sneed.*']\n",
    "    compiled_pc = [re.compile(patt) for patt in pattern_pc]\n",
    "    \n",
    "    # pattern list P_b rules for subtask B (and possibly the same for subtask A)\n",
    "    # pattern list P_b rules for subtask A\n",
    "    pattern_pb = [r'.*do\\snot.*', r'.*if\\sonly.*', r'.*(so|before|can|for|if)\\syou.*', \n",
    "                   r'.*you\\s(will|need|can|may).*', r'.*(make|be)\\ssure.*', r'.*watch\\sout.*', \n",
    "                   r'.*(go|going|asking|wishing)\\sfor.*', r'.*would\\sadvise.*', \n",
    "                   r'.*(will|would|could)\\sbe.*', r'.*be\\s(prepared|careful|warned|forewarned).*',\n",
    "                   r'.*(i/would/i\\'d)\\s(like|prefer).*', r'.*highly\\srecommended.*', \n",
    "                   r'.*(look|looking)\\s(into|for|up|around).*', r'.*why\\snot.*', r'.*is\\sthere.*',\n",
    "                   r'.*we\\sneed.*']\n",
    "    compiled_pb = [re.compile(patt) for patt in pattern_pb]\n",
    "        \n",
    "    pos_pattern_strings = [r'^UH\\sVBP.*', r'^MD\\sRB\\sPRP.*', r'^(VB|VBP).*', r'^MD.*', \n",
    "                           r'^(DT|RB|PRP|NN)\\sVB.*']\n",
    "    compiled_pos_patterns = [re.compile(patt) for patt in pos_pattern_strings]\n",
    "\n",
    "\n",
    "    label_list = []\n",
    "    for sent in sent_list:\n",
    "        score = 0\n",
    "        \n",
    "        if len(sent.split()) < 5:\n",
    "            score -=0.2\n",
    "        \n",
    "        clause_split = [a for a in re.split(\"[.,!?;]|(please)\", sent) if a not in \n",
    "                        [None, '', ' ', 'please']]\n",
    "        for clause in clause_split:\n",
    "            clause_pos = TextBlob(clause).tags\n",
    "            \n",
    "            words = [i[0] for i in clause_pos]\n",
    "            tags = [i[1] for i in clause_pos]\n",
    "            \n",
    "            # Correct misspells\n",
    "            if spelling:\n",
    "                words = [spell.correction(w) if w not in spell else w for w in words]\n",
    "            \n",
    "            if P_a:            \n",
    "                # Pattern P_a\n",
    "                if any(elem in pattern_pa for elem in words):\n",
    "                    score += 0.25\n",
    "\n",
    "\n",
    "            if P_b:\n",
    "                # Pattern P_b\n",
    "                for compiled_patt in compiled_pb:\n",
    "                    joined_sent = \" \".join(words)\n",
    "                    matches = compiled_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += 0.1\n",
    "            \n",
    "            if P_c:\n",
    "                # Pattern P_c\n",
    "                for compiled_patt in compiled_pc:\n",
    "                    joined_sent = \" \".join(words)\n",
    "                    matches = compiled_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += 0.25\n",
    "\n",
    "            if imperative:\n",
    "                # Imperative POS pattern check\n",
    "                for compiled_pos_patt in compiled_pos_patterns:\n",
    "                    joined_sent = \" \".join(tags)\n",
    "                    matches = compiled_pos_patt.findall(joined_sent)\n",
    "                    if len(matches) > 0:\n",
    "                        score += pos_s\n",
    "\n",
    "        if score > 0.1:\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.455\n",
      "MCC:  0.072\n",
      "Acc:  0.516\n",
      "Precision:  0.365\n",
      "Recall:  0.604\n"
     ]
    }
   ],
   "source": [
    "dev_pred_labels_b = gr_classify_all(dev_sentences, pos_s=0.15)\n",
    "print(\"F1:\", np.round(f1(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"Acc: \", np.round(acc(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"Precision: \", np.round(prec(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"Recall: \", np.round(rec(dev_labels_DS, dev_pred_labels_b), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need Advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'needadvice'\n",
    "\n",
    "train = pd.read_csv('../../annotated_data/' + data + '_train.tsv', sep='\\t', header=0)\n",
    "train['Sentence'] = train['Sentence'].apply(lambda x: x.lower())\n",
    "train_sentences = train['Sentence'].tolist()\n",
    "train_labels_DS = train['DS_Label'].values\n",
    "train_labels_Maj = train['Majority_label'].values\n",
    "\n",
    "\n",
    "dev = pd.read_csv('../../annotated_data/' + data + '_dev.tsv', sep='\\t', header=0)\n",
    "dev['Sentence'] = dev['Sentence'].apply(lambda x: x.lower())\n",
    "dev_sentences = dev['Sentence'].tolist()\n",
    "dev_labels_DS = dev['DS_Label'].values\n",
    "dev_labels_Maj = dev['Majority_label'].values\n",
    "\n",
    "test = pd.read_csv('../../annotated_data/' + data + '_test.tsv', sep='\\t', header=0)\n",
    "test['Sentence'] = test['Sentence'].apply(lambda x: x.lower())\n",
    "test_sentences_na = test['Sentence'].tolist()\n",
    "test_labels_DS_na = test['DS_Label'].values\n",
    "test_labels_Maj_na = test['Majority_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is advice, 0 is not.\n",
      "Distribution of Train set: Counter({0: 3851, 1: 2297}) 0.37\n",
      "Distribution of Dev set: Counter({0: 542, 1: 274}) 0.34\n",
      "Distribution of Test set: Counter({0: 568, 1: 330}) 0.37\n"
     ]
    }
   ],
   "source": [
    "print(\"1 is advice, 0 is not.\")\n",
    "print(\"Distribution of Train set:\", Counter(train_labels_DS), np.round(Counter(train_labels_DS)[1]/len(train_labels_DS),2))\n",
    "print(\"Distribution of Dev set:\", Counter(dev_labels_DS),  np.round(Counter(dev_labels_DS)[1]/len(dev_labels_DS), 2))\n",
    "print(\"Distribution of Test set:\", Counter(test_labels_DS_na),  np.round(Counter(test_labels_DS_na)[1]/len(test_labels_DS_na), 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEMEVAL Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.557\n",
      "MCC:  0.248\n",
      "Acc:  0.559\n",
      "Precision:  0.42\n",
      "Recall:  0.825\n"
     ]
    }
   ],
   "source": [
    "dev_pred_labels_baseline = classify(dev_sentences)\n",
    "print(\"F1:\", np.round(f1(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"Acc: \", np.round(acc(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"Precision: \", np.round(prec(dev_labels_DS, dev_pred_labels_baseline), sigdig))\n",
    "print(\"Recall: \", np.round(rec(dev_labels_DS, dev_pred_labels_baseline), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTUA-IS rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.483\n",
      "MCC:  0.098\n",
      "Acc:  0.506\n",
      "Precision:  0.372\n",
      "Recall:  0.686\n"
     ]
    }
   ],
   "source": [
    "dev_pred_labels_b = gr_classify_all(dev_sentences, pos_s=0.15)\n",
    "print(\"F1:\", np.round(f1(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"Acc: \", np.round(acc(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"Precision: \", np.round(prec(dev_labels_DS, dev_pred_labels_b), sigdig))\n",
    "print(\"Recall: \", np.round(rec(dev_labels_DS, dev_pred_labels_b), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Askparents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEMEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.446\n",
      "MCC:  0.169\n",
      "Acc:  0.544\n",
      "Precision:  0.327\n",
      "Recall:  0.702\n"
     ]
    }
   ],
   "source": [
    "test_pred_labels_bs_ap = classify(test_sentences_ap)\n",
    "print(\"F1:\", np.round(f1(test_labels_DS_ap, test_pred_labels_bs_ap), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(test_labels_DS_ap, test_pred_labels_bs_ap), sigdig))\n",
    "print(\"Acc: \", np.round(acc(test_labels_DS_ap, test_pred_labels_bs_ap), sigdig))\n",
    "print(\"Precision: \", np.round(prec(test_labels_DS_ap, test_pred_labels_bs_ap), sigdig))\n",
    "print(\"Recall: \", np.round(rec(test_labels_DS_ap, test_pred_labels_bs_ap), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTUA-IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.423\n",
      "MCC:  0.129\n",
      "Acc:  0.537\n",
      "Precision:  0.314\n",
      "Recall:  0.649\n"
     ]
    }
   ],
   "source": [
    "test_pred_labels_b_ap = gr_classify_all(test_sentences_ap, pos_s=0.15)\n",
    "print(\"F1:\", np.round(f1(test_labels_DS_ap, test_pred_labels_b_ap), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(test_labels_DS_ap, test_pred_labels_b_ap), sigdig))\n",
    "print(\"Acc: \", np.round(acc(test_labels_DS_ap, test_pred_labels_b_ap), sigdig))\n",
    "print(\"Precision: \", np.round(prec(test_labels_DS_ap, test_pred_labels_b_ap), sigdig))\n",
    "print(\"Recall: \", np.round(rec(test_labels_DS_ap, test_pred_labels_b_ap), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeedAdvice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEMEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.572\n",
      "MCC:  0.225\n",
      "Acc:  0.559\n",
      "Precision:  0.445\n",
      "Recall:  0.803\n"
     ]
    }
   ],
   "source": [
    "test_pred_labels_bs_na = classify(test_sentences_na)\n",
    "print(\"F1:\", np.round(f1(test_labels_DS_na, test_pred_labels_bs_na), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(test_labels_DS_na, test_pred_labels_bs_na), sigdig))\n",
    "print(\"Acc: \", np.round(acc(test_labels_DS_na, test_pred_labels_bs_na), sigdig))\n",
    "print(\"Precision: \", np.round(prec(test_labels_DS_na, test_pred_labels_bs_na), sigdig))\n",
    "print(\"Recall: \", np.round(rec(test_labels_DS_na, test_pred_labels_bs_na), sigdig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTUA-IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.535\n",
      "MCC:  0.161\n",
      "Acc:  0.548\n",
      "Precision:  0.43\n",
      "Recall:  0.709\n"
     ]
    }
   ],
   "source": [
    "test_pred_labels_b_na = gr_classify_all(test_sentences_na, pos_s=0.15)\n",
    "print(\"F1:\", np.round(f1(test_labels_DS_na, test_pred_labels_b_na), sigdig))\n",
    "print(\"MCC: \", np.round(mcc(test_labels_DS_na, test_pred_labels_b_na), sigdig))\n",
    "print(\"Acc: \", np.round(acc(test_labels_DS_na, test_pred_labels_b_na), sigdig))\n",
    "print(\"Precision: \", np.round(prec(test_labels_DS_na, test_pred_labels_b_na), sigdig))\n",
    "print(\"Recall: \", np.round(rec(test_labels_DS_na, test_pred_labels_b_na), sigdig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 544, 0: 354})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_pred_labels_b_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
