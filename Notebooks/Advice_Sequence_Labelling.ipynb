{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipdb\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, accuracy_score as acc, precision_score as prec, recall_score as rec, matthews_corrcoef as mattcorr\n",
    "import torch\n",
    "from transformers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x1A4ABC1678"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seeds for reproducibility on a specific machine\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../annotated_data/train.tsv', sep='\\t', header=0)\n",
    "train['Sentence'] = train['Sentence'].apply(lambda x: x.lower())\n",
    "train_sentences = train['Sentence'].tolist()\n",
    "train_labels_DS = train['DS_Label'].values\n",
    "train_labels_Maj = train['Majority_label'].values\n",
    "\n",
    "\n",
    "dev = pd.read_csv('../annotated_data/dev.tsv', sep='\\t', header=0)\n",
    "dev['Sentence'] = dev['Sentence'].apply(lambda x: x.lower())\n",
    "dev_sentences = dev['Sentence'].tolist()\n",
    "dev_labels_DS = dev['DS_Label'].values\n",
    "dev_labels_Maj = dev['Majority_label'].values\n",
    "\n",
    "test = pd.read_csv('../annotated_data/test.tsv', sep='\\t', header=0)\n",
    "test['Sentence'] = test['Sentence'].apply(lambda x: x.lower())\n",
    "test_sentences = test['Sentence'].tolist()\n",
    "test_labels_DS = test['DS_Label'].values\n",
    "test_labels_Maj = test['Majority_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new columns that might help later on with the sequence labelling task:\n",
    "1. *Post.ID* The reddit post from which the replies are scraped - \n",
    "2. *Reply.ID* The id number of the reply, as ordered by reddit's best algorithm (need to check up on this with Ben)\n",
    "3. *Sent.Num* The sentence number, in order, from within the reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Majority_label</th>\n",
       "      <th>DS_Label</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Post.ID</th>\n",
       "      <th>Reply.ID</th>\n",
       "      <th>Sent.Num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dbk05v-8-2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>if you had an office ( is studio the right wor...</td>\n",
       "      <td>dbk05v</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8pf0i0-2-4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>but i would not go to a strip club with my mom .</td>\n",
       "      <td>8pf0i0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cqeljh-4-0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i 'm not in your situation .</td>\n",
       "      <td>cqeljh</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb4rzb-1-5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>if your home is half way clean , oh yeah , the...</td>\n",
       "      <td>cb4rzb</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gvjhg-4-0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>same as everyone else .</td>\n",
       "      <td>7gvjhg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Majority_label  DS_Label  \\\n",
       "ID                                     \n",
       "dbk05v-8-2               1         1   \n",
       "8pf0i0-2-4               0         0   \n",
       "cqeljh-4-0               0         0   \n",
       "cb4rzb-1-5               1         1   \n",
       "7gvjhg-4-0               0         0   \n",
       "\n",
       "                                                     Sentence Post.ID  \\\n",
       "ID                                                                      \n",
       "dbk05v-8-2  if you had an office ( is studio the right wor...  dbk05v   \n",
       "8pf0i0-2-4   but i would not go to a strip club with my mom .  8pf0i0   \n",
       "cqeljh-4-0                       i 'm not in your situation .  cqeljh   \n",
       "cb4rzb-1-5  if your home is half way clean , oh yeah , the...  cb4rzb   \n",
       "7gvjhg-4-0                            same as everyone else .  7gvjhg   \n",
       "\n",
       "           Reply.ID Sent.Num  \n",
       "ID                            \n",
       "dbk05v-8-2        8        2  \n",
       "8pf0i0-2-4        2        4  \n",
       "cqeljh-4-0        4        0  \n",
       "cb4rzb-1-5        1        5  \n",
       "7gvjhg-4-0        4        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Post.ID'] = train['ID'].apply(lambda x: x.split('-')[0])\n",
    "train['Reply.ID'] = train['ID'].apply(lambda x: x.split('-')[1])\n",
    "train['Sent.Num'] = train['ID'].apply(lambda x: x.split('-')[2])\n",
    "\n",
    "dev['Post.ID'] = dev['ID'].apply(lambda x: x.split('-')[0])\n",
    "dev['Reply.ID'] = dev['ID'].apply(lambda x: x.split('-')[1])\n",
    "dev['Sent.Num'] = dev['ID'].apply(lambda x: x.split('-')[2])\n",
    "\n",
    "test['Post.ID'] = test['ID'].apply(lambda x: x.split('-')[0])\n",
    "test['Reply.ID'] = test['ID'].apply(lambda x: x.split('-')[1])\n",
    "test['Sent.Num'] = test['ID'].apply(lambda x: x.split('-')[2])\n",
    "\n",
    "train.set_index('ID',inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91, 92, 92, 93, 112, 126, 126, 129, 139, 152]\n",
      "91\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum length of sequences in train, dev and test -- useful for setting MAX_LEN later\n",
    "print(sorted([len(a.split()) for a in train_sentences])[-10:])\n",
    "print(max([len(a.split()) for a in dev_sentences]))\n",
    "print(max([len(a.split()) for a in test_sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models are the ones I need to look into for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [(BertModel,       BertTokenizer,       'bert-base-cased'),\n",
    "          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),\n",
    "          (RobertaModel,    RobertaTokenizer,    'roberta-base')\n",
    "         ]\n",
    "# In addition, need to look into SentenceBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Labelling Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Majority_label</th>\n",
       "      <th>DS_Label</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Post.ID</th>\n",
       "      <th>Reply.ID</th>\n",
       "      <th>Sent.Num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30twyy-1-0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>i would ask for a thyroid check .</td>\n",
       "      <td>30twyy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30twyy-1-1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>no need for a specialist there .</td>\n",
       "      <td>30twyy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30twyy-1-2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>as for the teeth , that 's better brought up w...</td>\n",
       "      <td>30twyy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30twyy-1-3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i 'm 26 years old and i still have a baby toot...</td>\n",
       "      <td>30twyy</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30twyy-1-4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>does n't necessarily mean anything bad health ...</td>\n",
       "      <td>30twyy</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Majority_label  DS_Label  \\\n",
       "ID                                     \n",
       "30twyy-1-0               1         1   \n",
       "30twyy-1-1               0         0   \n",
       "30twyy-1-2               1         1   \n",
       "30twyy-1-3               0         0   \n",
       "30twyy-1-4               0         0   \n",
       "\n",
       "                                                     Sentence Post.ID  \\\n",
       "ID                                                                      \n",
       "30twyy-1-0                  i would ask for a thyroid check .  30twyy   \n",
       "30twyy-1-1                   no need for a specialist there .  30twyy   \n",
       "30twyy-1-2  as for the teeth , that 's better brought up w...  30twyy   \n",
       "30twyy-1-3  i 'm 26 years old and i still have a baby toot...  30twyy   \n",
       "30twyy-1-4  does n't necessarily mean anything bad health ...  30twyy   \n",
       "\n",
       "           Reply.ID Sent.Num  \n",
       "ID                            \n",
       "30twyy-1-0        1        0  \n",
       "30twyy-1-1        1        1  \n",
       "30twyy-1-2        1        2  \n",
       "30twyy-1-3        1        3  \n",
       "30twyy-1-4        1        4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sort_values(by=['Post.ID', 'Reply.ID', 'Sent.Num']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO_convert(array):\n",
    "    '''\n",
    "    Convert a sequence of 1s and 0s to BIO(Beginning-Inside-Outside) format\n",
    "    '''\n",
    "    bio = ['O' for i in range(len(array))]\n",
    "    if 1 not in array:\n",
    "        return bio\n",
    "    else:\n",
    "        bio[array.index(1)] = 'B'\n",
    "        for k in range(array.index(1)+1, len(array)):\n",
    "            if array[k] == 1 and bio[k-1] == 'B':\n",
    "                bio[k] = 'I'\n",
    "            elif array[k] == 0:\n",
    "                bio[k] = 'O'\n",
    "            elif array[k] == 1 and array[k-1] == 0:\n",
    "                bio[k] = 'B'\n",
    "        return bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Post.Reply.ID'] = train.apply(lambda row: row['Post.ID'] + '-' + str(row['Reply.ID']), axis=1)\n",
    "\n",
    "train_seq = {'Post.Reply.ID':[], 'Sequence':[], 'Maj_Seq_Labels': [], 'DS_Seq_Labels': [] }\n",
    "\n",
    "for reply_id in train['Post.Reply.ID'].unique():\n",
    "    train_seq['Post.Reply.ID'].append(reply_id)\n",
    "    train_seq['Sequence'].append(\n",
    "        train[train['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['Sentence'].values.tolist())\n",
    "    train_seq['Maj_Seq_Labels'].append( \n",
    "        train[train['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['Majority_label'].values.tolist())\n",
    "    train_seq['DS_Seq_Labels'].append(\n",
    "        train[train['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['DS_Label'].values.tolist())\n",
    "        \n",
    "train_seq = pd.DataFrame(train_seq)\n",
    "train_seq['Sequence'] = train_seq['Sequence'].apply(lambda x: '[CLS] ' + ' [CLS] '.join(x))\n",
    "\n",
    "# train[train['Post.Reply.ID']=='30twyy-1'].sort_values(by=['Sent.Num'])['Sentence'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert Majority and DS Labels to BIO format\n",
    "# train_seq['Maj_Seq_Labels'] = train_seq['Maj_Seq_Labels'].apply(lambda x: BIO_convert(x))\n",
    "# train_seq['DS_Seq_Labels'] = train_seq['DS_Seq_Labels'].apply(lambda x: BIO_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same as above, but for dev set\n",
    "dev['Post.Reply.ID'] = dev.apply(lambda row: row['Post.ID'] + '-' + str(row['Reply.ID']), axis=1)\n",
    "\n",
    "dev_seq = {'Post.Reply.ID':[], 'Sequence':[], 'Maj_Seq_Labels': [], 'DS_Seq_Labels': [] }\n",
    "\n",
    "for reply_id in dev['Post.Reply.ID'].unique():\n",
    "    dev_seq['Post.Reply.ID'].append(reply_id)\n",
    "    dev_seq['Sequence'].append(\n",
    "        dev[dev['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['Sentence'].values.tolist())\n",
    "    dev_seq['Maj_Seq_Labels'].append( \n",
    "        dev[dev['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['Majority_label'].values.tolist())\n",
    "    dev_seq['DS_Seq_Labels'].append(\n",
    "        dev[dev['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['DS_Label'].values.tolist())\n",
    "        \n",
    "dev_seq = pd.DataFrame(dev_seq)\n",
    "dev_seq['Sequence'] = dev_seq['Sequence'].apply(lambda x: '[CLS] ' + ' [CLS] '.join(x))\n",
    "\n",
    "## Convert Majority and DS Labels to BIO format\n",
    "# dev_seq['Maj_Seq_Labels'] = dev_seq['Maj_Seq_Labels'].apply(lambda x: BIO_convert(x))\n",
    "# dev_seq['DS_Seq_Labels'] = dev_seq['DS_Seq_Labels'].apply(lambda x: BIO_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same as above, but for test set\n",
    "test['Post.Reply.ID'] = test.apply(lambda row: row['Post.ID'] + '-' + str(row['Reply.ID']), axis=1)\n",
    "\n",
    "test_seq = {'Post.Reply.ID':[], 'Sequence':[], 'Maj_Seq_Labels': [], 'DS_Seq_Labels': [] }\n",
    "\n",
    "for reply_id in test['Post.Reply.ID'].unique():\n",
    "    test_seq['Post.Reply.ID'].append(reply_id)\n",
    "    test_seq['Sequence'].append(\n",
    "        test[test['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['Sentence'].values.tolist())\n",
    "    test_seq['Maj_Seq_Labels'].append( \n",
    "        test[test['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['Majority_label'].values.tolist())\n",
    "    test_seq['DS_Seq_Labels'].append(\n",
    "        test[test['Post.Reply.ID']==reply_id].sort_values(by=['Sent.Num'])['DS_Label'].values.tolist())\n",
    "        \n",
    "test_seq = pd.DataFrame(test_seq)\n",
    "test_seq['Sequence'] = test_seq['Sequence'].apply(lambda x: '[CLS] ' + ' [CLS] '.join(x))\n",
    "\n",
    "## Convert Majority and DS Labels to BIO format\n",
    "# test_seq['Maj_Seq_Labels'] = test_seq['Maj_Seq_Labels'].apply(lambda x: BIO_convert(x))\n",
    "# test_seq['DS_Seq_Labels'] = test_seq['DS_Seq_Labels'].apply(lambda x: BIO_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8623, 7) (2288, 4) (1309, 8) (336, 4) (1043, 8) (285, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, train_seq.shape, dev.shape, dev_seq.shape, test.shape, test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "163\n",
      "161\n",
      "16\n",
      "15\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(max([len(x.split()) for x in train_seq['Sequence']]))\n",
    "print(max([len(x.split()) for x in dev_seq['Sequence']]))\n",
    "print(max([len(x.split()) for x in test_seq['Sequence']]))\n",
    "\n",
    "\n",
    "print(max([len(x) for x in train_seq['DS_Seq_Labels']]))\n",
    "print(max([len(x) for x in dev_seq['DS_Seq_Labels']]))\n",
    "print(max([len(x) for x in test_seq['DS_Seq_Labels']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a [CLS] to the beginning of each sentence in sequence(reply). Then extract that and do one of the following below:\n",
    "\n",
    "Put a **CRF** on top of a list of representations for each sentence - the representation could be CLS Token\n",
    "\n",
    "Or put a simple **LSTM/GRU** on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "MAX_LEN = 256\n",
    "MAX_SEQ_LEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_len(a):\n",
    "    '''\n",
    "    Finds the length of the sequence beyond which padding is necessary (in the batch of cls tokens)\n",
    "    a: tensor of shape max_len\n",
    "    \n",
    "    returns an integer (which is length for that sequence - this is equivalent to number of sentences in\n",
    "    a reply)\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        len = ((a != 0).nonzero())[-1].item() + 1\n",
    "    except IndexError:\n",
    "        len = 1\n",
    "    \n",
    "    return len\n",
    "\n",
    "# Convert our text into tokens that corresponds to BERT library\n",
    "train_input_ids = [tokenizer.encode(seq, max_length=MAX_LEN,pad_to_max_length=True,add_special_tokens=False) \n",
    "                   for seq in train_seq['Sequence'].tolist()]\n",
    "train_input_ids = torch.tensor(train_input_ids)\n",
    "\n",
    "\n",
    "# Create a mask of 1 for all input tokens and 0 for all padding tokens \n",
    "train_attention_masks = torch.tensor([[float(i>0) for i in seq] for seq in train_input_ids])\n",
    "\n",
    "# Find the indicies of all [CLS] tokens that will be used later for with RNN/CRF on top\n",
    "train_cls_idxs = torch.nn.utils.rnn.pad_sequence([torch.tensor(np.where(a==101)[0]) for a in train_input_ids.numpy()], padding_value=0, batch_first=True)\n",
    "# import ipdb;ipdb.set_trace()\n",
    "train_cls_lens = torch.tensor([find_len(a) for a in train_cls_idxs])\n",
    "\n",
    "# Pad the labels with 0\n",
    "train_labels_DS = torch.nn.utils.rnn.pad_sequence([torch.tensor(a) for a in train_seq['DS_Seq_Labels'].values], padding_value=-1, batch_first=True)\n",
    "\n",
    "# Same for dev\n",
    "dev_input_ids = [tokenizer.encode(seq, max_length=MAX_LEN,pad_to_max_length=True,add_special_tokens=False) \n",
    "                 for seq in dev_seq['Sequence'].tolist()]\n",
    "dev_input_ids = torch.tensor(dev_input_ids)\n",
    "\n",
    "\n",
    "dev_attention_masks = torch.tensor([[float(i>0) for i in seq] for seq in dev_input_ids])\n",
    "\n",
    "dev_cls_idxs = torch.nn.utils.rnn.pad_sequence([torch.tensor(np.where(a==101)[0]) for a in dev_input_ids.numpy()], padding_value=0, batch_first=True)\n",
    "dev_cls_lens = torch.tensor([find_len(a) for a in dev_cls_idxs])\n",
    "\n",
    "dev_labels_DS = torch.nn.utils.rnn.pad_sequence([torch.tensor(a) for a in dev_seq['DS_Seq_Labels'].values], padding_value=-1, batch_first=True)\n",
    "\n",
    "print(train_input_ids.shape, train_attention_masks.shape, train_labels_DS.shape, train_cls_idxs.shape)\n",
    "print(dev_input_ids.shape, dev_attention_masks.shape, dev_labels_DS.shape, dev_cls_idxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_cls_idxs[0],\"\\n\", train_cls_lens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_input_ids,train_attention_masks,train_labels_DS, train_cls_idxs, \n",
    "                           train_cls_lens)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "dev_data = TensorDataset(dev_input_ids,dev_attention_masks,dev_labels_DS, dev_cls_idxs, dev_cls_lens)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data,sampler=dev_sampler,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup all the parameters\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.cuda.device(0)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, transformer_config, device, input_size, hidden_size, num_layers, dropout, layers):\n",
    "        '''\n",
    "        Setup the modules in the model - a transformer, followed by a GRU for the CLS hidden states/taking\n",
    "        the mean of all tokens, followed by Linear layers that outputs one number, followed by softmax\n",
    "        '''\n",
    "        super(SequentialModel, self).__init__()\n",
    "        \n",
    "        # Setup the transformer and the GRU layer on top of the CLS tokens\n",
    "        self._transformer = BertModel.from_pretrained(transformer_config)\n",
    "        self._rnn = torch.nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, \n",
    "                                 batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        # Setup the linear layers on top of the GRU final hidden state\n",
    "        output_size = 2\n",
    "        self._linmaps = torch.nn.ModuleList([])\n",
    "        last_size = hidden_size * 2\n",
    "        for j in layers:\n",
    "            self._linmaps.append(torch.nn.Linear(last_size, j))\n",
    "            last_size = j\n",
    "        self._linmaps.append(torch.nn.Linear(last_size, output_size))\n",
    "        \n",
    "        self._activation = 'relu'\n",
    "        self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        self._logsoftmax = torch.nn.LogSoftmax(dim=2)\n",
    "        \n",
    "    def nonlinearity(self, x):\n",
    "        '''Applies relu or tanh activation on tensor.'''\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            return torch.nn.functional.relu(x)\n",
    "        elif self._activation == 'tanh':\n",
    "            return torch.tanh(x)\n",
    "    \n",
    "    def forward(self, input_ids, input_mask, input_cls_idxs, input_cls_lens, max_seq_len):\n",
    "        '''\n",
    "        Runs forward pass on neural network\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        input_ids: the tokenized, bert wordpiece IDs. (batch_size, MAX_LEN)\n",
    "        input_masks: the masking to be done on input_ids due to padding. (batch_size, MAX_LEN)\n",
    "        input_cls_idxs: the indicies of the CLS tokens for each sequence. (batch_size, max_num_seq)\n",
    "        input_cls_lens: the length of each sequence in the batch (for packing before passing through GRU)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # Forward pass through transformer\n",
    "        # other values returned are pooler_output, hidden_states, and attentions\n",
    "        last_hidden_state, _ = self._transformer(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "\n",
    "        # last_hidden_state is of shape batch_size x MAX_LEN(256) x 768 (for bert-base-cased)\n",
    "        # now extract along the cls_idx for each sequence\n",
    "        # cls_hidden_states will be of shape batch_size x max_num_of_sequences x 768\n",
    "        cls_hidden_states = torch.cat([torch.index_select(a, 0, i).unsqueeze(0) for a, i in \n",
    "                                       zip(last_hidden_state, input_cls_idxs) ])\n",
    "        \n",
    "        # Else put in something here to take the mean of the whole sequence instead\n",
    "\n",
    "        \n",
    "        # Pack padded sequence above\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(cls_hidden_states,input_cls_lens,enforce_sorted=False, \n",
    "                                                    batch_first=True)\n",
    "        \n",
    "\n",
    "        # Run it through the GRU\n",
    "        x, _ = self._rnn(x)\n",
    "        \n",
    "        # Unpack packed sequence\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, total_length=max_seq_len)\n",
    "\n",
    "        # Then run it through linear layers\n",
    "        # Run through linear layers\n",
    "        for i, linmap in enumerate(self._linmaps):\n",
    "            if i:\n",
    "                x = self.nonlinearity(x)\n",
    "                x = self._dropout(x)\n",
    "            x = linmap(x)\n",
    "        \n",
    "        x = self._logsoftmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequentialModel(transformer_config='bert-base-cased', device=device, input_size=768, hidden_size=256, \n",
    "                        num_layers=2, dropout=0.2, layers=(64,16)).to(device)\n",
    "\n",
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, y_pred):\n",
    "    '''\n",
    "    Calculates cross entropy loss\n",
    "    \n",
    "    Arguments:\n",
    "    =========\n",
    "    y: actual outputs. (batch_size x max_seq_len)\n",
    "    y_pred: model's predicted outputs. (batch_size x max_seq_len)\n",
    "    \n",
    "    Returns:\n",
    "    =======\n",
    "    loss: cross entropy loss / binary cross entropy loss for sequence\n",
    "    '''\n",
    "    \n",
    "    # Flatten the true and predicted labels\n",
    "    y = y.view(-1)\n",
    "    y_pred = y_pred.view(-1,2)\n",
    "\n",
    "    \n",
    "    # First figure out which values to mask from loss calculation (padded with -1)\n",
    "    mask = (y > -1).float()\n",
    "    \n",
    "    # Calculate cross entropy loss manually because of masking\n",
    "    num_of_tokens = int(torch.sum(mask).item())\n",
    "    \n",
    "    # Ok this is a little hard to read. First y_pred[range(y_pred.shape[0]), y] returns a (512,) tensor\n",
    "    # that looks at every element in y_pred and returns the 0th or 1th element corresponding to what is in\n",
    "    # the actual label (is this what cross entropy does? I think y.p(y) + (1-y)p(1-y)). However -1 will\n",
    "    # choose the last element for those elements as well. Then multipy by  mask to zero out all the -1 labels. \n",
    "    y_pred = y_pred[range(y_pred.shape[0]), y] * mask\n",
    "    \n",
    "    # Now calculate loss by just summing it up\n",
    "    loss = -torch.sum(y_pred)/num_of_tokens\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "for _ in range(1,epochs+1):\n",
    "    print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "    \n",
    "    # Calculate total loss for this epoch\n",
    "    batch_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        model.train()\n",
    "        \n",
    "         # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels, b_cls_idxs, b_cls_lens = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        b_outputs = model(input_ids=b_input_ids, input_mask=b_input_mask, input_cls_idxs=b_cls_idxs,\n",
    "                          input_cls_lens=b_cls_lens, max_seq_len=16)\n",
    "\n",
    "        loss = loss_fn(b_labels, b_outputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # Gradient clipping is not in AdamW anymore\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Update learning rate schedule\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear the previous accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update tracking variables\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "    #store the current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "        learning_rate.append(param_group['lr'])\n",
    "\n",
    "    train_loss_set.append(avg_train_loss)\n",
    "    print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_accuracy, eval_mcc_accuracy, eval_f1_score, nb_eval_steps = 0, 0, 0, 0\n",
    "\n",
    "    for batch in dev_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels, b_cls_idxs, b_cls_lens = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "          # Forward pass, calculate logit predictions\n",
    "          b_outputs = model(input_ids=b_input_ids, input_mask=b_input_mask, input_cls_idxs=b_cls_idxs,\n",
    "                            input_cls_lens=b_cls_lens, max_seq_len=15)\n",
    "\n",
    "        \n",
    "        b_outputs = b_outputs.to('cpu').numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(b_outputs, axis=1).flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        mask = (labels_flat > -1).int()\n",
    "    \n",
    "        tmp_eval_accuracy = acc(pred_flat, labels_flat, sample_weight=mask)\n",
    "        tmp_eval_mcc_accuracy = mattcorr(labels_flat, pred_flat, sample_weight=mask)\n",
    "        tmp_eval_f1_score = f1_score(labels_flat, pred_flat, sample_weight=mask)\n",
    "\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "        eval_f1_score += tmp_eval_f1_score\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "    print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\n",
    "    print(F'\\n\\tValidation F1 Score: {eval_f1_score/nb_eval_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
