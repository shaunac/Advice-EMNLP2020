{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, accuracy_score as acc, precision_score as prec, recall_score as rec\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, XLNetForSequenceClassification, XLNetTokenizer, RobertaForSequenceClassification, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x1A3DF07570"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seeds for reproducibility on a specific machine\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../annotated_data/train.tsv', sep='\\t', header=0)\n",
    "train['Sentence'] = train['Sentence'].apply(lambda x: x.lower())\n",
    "train_sentences = train['Sentence'].tolist()\n",
    "train_labels_DS = train['DS_Label'].values\n",
    "train_labels_Maj = train['Majority_label'].values\n",
    "\n",
    "\n",
    "dev = pd.read_csv('../annotated_data/dev.tsv', sep='\\t', header=0)\n",
    "dev['Sentence'] = dev['Sentence'].apply(lambda x: x.lower())\n",
    "dev_sentences = dev['Sentence'].tolist()\n",
    "dev_labels_DS = dev['DS_Label'].values\n",
    "dev_labels_Maj = dev['Majority_label'].values\n",
    "\n",
    "test = pd.read_csv('../annotated_data/test.tsv', sep='\\t', header=0)\n",
    "test['Sentence'] = test['Sentence'].apply(lambda x: x.lower())\n",
    "test_sentences = test['Sentence'].tolist()\n",
    "test_labels_DS = test['DS_Label'].values\n",
    "test_labels_Maj = test['Majority_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new columns that might help later on with the sequence labelling task:\n",
    "1. *Post.ID* The reddit post from which the replies are scraped - \n",
    "2. *Reply.ID* The id number of the reply, as ordered by reddit's best algorithm (need to check up on this with Ben)\n",
    "3. *Sent.Num* The sentence number, in order, from within the reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Majority_label</th>\n",
       "      <th>DS_Label</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Post.ID</th>\n",
       "      <th>Reply.ID</th>\n",
       "      <th>Sent.Num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dbk05v-8-2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>if you had an office ( is studio the right wor...</td>\n",
       "      <td>dbk05v</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8pf0i0-2-4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>but i would not go to a strip club with my mom .</td>\n",
       "      <td>8pf0i0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cqeljh-4-0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>i 'm not in your situation .</td>\n",
       "      <td>cqeljh</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb4rzb-1-5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>if your home is half way clean , oh yeah , the...</td>\n",
       "      <td>cb4rzb</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7gvjhg-4-0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>same as everyone else .</td>\n",
       "      <td>7gvjhg</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Majority_label  DS_Label  \\\n",
       "ID                                     \n",
       "dbk05v-8-2               1         1   \n",
       "8pf0i0-2-4               0         0   \n",
       "cqeljh-4-0               0         0   \n",
       "cb4rzb-1-5               1         1   \n",
       "7gvjhg-4-0               0         0   \n",
       "\n",
       "                                                     Sentence Post.ID  \\\n",
       "ID                                                                      \n",
       "dbk05v-8-2  if you had an office ( is studio the right wor...  dbk05v   \n",
       "8pf0i0-2-4   but i would not go to a strip club with my mom .  8pf0i0   \n",
       "cqeljh-4-0                       i 'm not in your situation .  cqeljh   \n",
       "cb4rzb-1-5  if your home is half way clean , oh yeah , the...  cb4rzb   \n",
       "7gvjhg-4-0                            same as everyone else .  7gvjhg   \n",
       "\n",
       "           Reply.ID Sent.Num  \n",
       "ID                            \n",
       "dbk05v-8-2        8        2  \n",
       "8pf0i0-2-4        2        4  \n",
       "cqeljh-4-0        4        0  \n",
       "cb4rzb-1-5        1        5  \n",
       "7gvjhg-4-0        4        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Post.ID'] = train['ID'].apply(lambda x: x.split('-')[0])\n",
    "train['Reply.ID'] = train['ID'].apply(lambda x: x.split('-')[1])\n",
    "train['Sent.Num'] = train['ID'].apply(lambda x: x.split('-')[2])\n",
    "\n",
    "dev['Post.ID'] = dev['ID'].apply(lambda x: x.split('-')[0])\n",
    "dev['Reply.ID'] = dev['ID'].apply(lambda x: x.split('-')[1])\n",
    "dev['Sent.Num'] = dev['ID'].apply(lambda x: x.split('-')[2])\n",
    "\n",
    "test['Post.ID'] = test['ID'].apply(lambda x: x.split('-')[0])\n",
    "test['Reply.ID'] = test['ID'].apply(lambda x: x.split('-')[1])\n",
    "test['Sent.Num'] = test['ID'].apply(lambda x: x.split('-')[2])\n",
    "\n",
    "train.set_index('ID',inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BIO_convert(array):\n",
    "#     '''\n",
    "#     Convert a sequence of 1s and 0s to BIO(Beginning-Inside-Outside) format\n",
    "#     '''\n",
    "#     bio = ['O' for i in range(len(array))]\n",
    "#     if 1 not in array:\n",
    "#         return bio\n",
    "#     else:\n",
    "#         bio[array.index(1)] = 'B'\n",
    "#         for k in range(array.index(1)+1, len(array)):\n",
    "#             if array[k] == 1 and bio[k-1] == 'B':\n",
    "#                 bio[k] = 'I'\n",
    "#             elif array[k] == 0:\n",
    "#                 bio[k] = 'O'\n",
    "#             elif array[k] == 1 and array[k-1] == 0:\n",
    "#                 bio[k] = 'B'\n",
    "#         return bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS = {'bert': (BertForSequenceClassification, BertTokenizer,\n",
    "#                  'bert-base-cased'),\n",
    "#           'xlnet': (XLNetForSequenceClassification, XLNetTokenizer,\n",
    "#                    'xlnet-base-cased'),\n",
    "#           'roberta': (RobertaForSequenceClassification, RobertaTokenizer,\n",
    "#                       'roberta-base')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Sequence Classification with Fine tuned BERT (DS labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the maximum length of sequence - just the longest length sentence from\n",
    "# # train, test and dev\n",
    "# MAX_LEN = max(max([len(a.split()) for a in train_sentences]),\n",
    "#               max([len(a.split()) for a in dev_sentences]),\n",
    "#               max([len(a.split()) for a in test_sentences]))\n",
    "\n",
    "# # Select a batch size for training. For fine-tuning xlnet on a specific task, the authors recommend a batch size of 16 or 32\n",
    "# batch_size = 32\n",
    "\n",
    "# # Choose gpu or cpu\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda:0')\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "    \n",
    "# # How many labels in your problem?\n",
    "# num_labels = np.unique(train_labels_DS).shape[0]\n",
    "\n",
    "# # Load tokenizer XForSequenceClassification model, the pretrained args.model\n",
    "# # with a single linear classification layer on top\n",
    "# tokenizer = MODELS['xlnet'][1].from_pretrained(MODELS['xlnet'][2])\n",
    "# model = MODELS['xlnet'][0].from_pretrained(MODELS['xlnet'][2], num_labels=num_labels).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'hidden_dropout_prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cc4735db5025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2, \n\u001b[0;32m----> 2\u001b[0;31m                                                       hidden_dropout_prob=0.5).to(device)\n\u001b[0m",
      "\u001b[0;32m~/opt/miniconda3/envs/advice/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'hidden_dropout_prob'"
     ]
    }
   ],
   "source": [
    "# model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2, \n",
    "#                                                       hidden_dropout_prob=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our text into tokens that corresponds to BERT library\n",
    "# train_input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) \n",
    "#                    for sent in train_sentences]\n",
    "# train_input_ids = torch.tensor(train_input_ids)\n",
    "\n",
    "# # Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "# train_attention_masks = [[float(i>0) for i in seq] for seq in train_input_ids]\n",
    "# train_attention_masks = torch.tensor(train_attention_masks)\n",
    "# train_labels_DS = torch.tensor(train_labels_DS)\n",
    "\n",
    "# # Same for dev\n",
    "# dev_input_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) \n",
    "#                  for sent in dev_sentences]\n",
    "# dev_input_ids = torch.tensor(dev_input_ids)\n",
    "# dev_attention_masks = [[float(i>0) for i in seq] for seq in dev_input_ids]\n",
    "# dev_attention_masks = torch.tensor(dev_attention_masks)\n",
    "# dev_labels_DS = torch.tensor(dev_labels_DS)\n",
    "\n",
    "# print(train_input_ids.shape, dev_input_ids.shape, train_attention_masks.shape, dev_attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n",
    "# train_data = TensorDataset(train_input_ids[:32],train_attention_masks[:32],train_labels_DS[:32])\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "# train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "# dev_data = TensorDataset(dev_input_ids[:32],dev_attention_masks[:32],dev_labels_DS[:32])\n",
    "# dev_sampler = RandomSampler(dev_data)\n",
    "# dev_dataloader = DataLoader(dev_data,sampler=dev_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2, \n",
    "#                                                       hidden_dropout_prob=0.5).to(device)\n",
    "\n",
    "# # Parameters:\n",
    "# lr = 2e-7\n",
    "# adam_epsilon = 1e-8\n",
    "\n",
    "# # Number of training epochs (authors recommend between 2 and 4)\n",
    "# epochs = 6\n",
    "\n",
    "# num_warmup_steps = 0\n",
    "# num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "# ### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "# optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Fine-tuning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "# train_loss_set = []\n",
    "# learning_rate = []\n",
    "# validation_acc = [0]\n",
    "\n",
    "# # Gradients gets accumulated by default\n",
    "# model.zero_grad()\n",
    "\n",
    "# for _ in range(1,epochs+1):\n",
    "#     print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "    \n",
    "#     # Calculate total loss for this epoch\n",
    "#     batch_loss = 0\n",
    "    \n",
    "#     for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "#         # Set our model to training mode (as opposed to evaluation mode)\n",
    "#         model.train()\n",
    "        \n",
    "#          # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         from IPython.core.debugger import set_trace;set_trace()\n",
    "#         # Forward pass\n",
    "#         outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # Clip the norm of the gradients to 1.0\n",
    "#         # Gradient clipping is not in AdamW anymore\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "#         # Update parameters and take a step using the computed gradient\n",
    "#         optimizer.step()\n",
    "    \n",
    "#         # Update learning rate schedule\n",
    "#         scheduler.step()\n",
    "\n",
    "#         # Clear the previous accumulated gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Update tracking variables\n",
    "#         batch_loss += loss.item()\n",
    "        \n",
    "#     # Calculate the average loss over the training data.\n",
    "#     avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "#     #store the current learning rate\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "#         learning_rate.append(param_group['lr'])\n",
    "\n",
    "#     train_loss_set.append(avg_train_loss)\n",
    "#     print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "\n",
    "#     # Put model in evaluation mode to evaluate loss on the validation set\n",
    "#     model.eval()\n",
    "\n",
    "#     # Tracking variables \n",
    "#     eval_accuracy, eval_f1_score, nb_eval_steps = 0, 0, 0\n",
    "\n",
    "#     for batch in dev_dataloader:\n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "#         # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "#         with torch.no_grad():\n",
    "#             # Forward pass, calculate logit predictions\n",
    "#             (logits,) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = logits.to('cpu').numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "#         pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "#         labels_flat = label_ids.flatten()\n",
    "#         tmp_eval_accuracy = acc(pred_flat, labels_flat)\n",
    "#         tmp_eval_f1_score = f1_score(labels_flat, pred_flat)\n",
    "\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "#         eval_f1_score += tmp_eval_f1_score\n",
    "#         nb_eval_steps += 1\n",
    "    \n",
    "#     validation_acc.append(eval_accuracy/nb_eval_steps)\n",
    "#     print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "# #     print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\n",
    "#     print(F'\\n\\tValidation F1 Score: {eval_f1_score/nb_eval_steps}')\n",
    "# #     if valid_acc[-1] < valid_acc[-2]:\n",
    "# #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
